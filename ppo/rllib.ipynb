{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_BoKmYo6oUci"
   },
   "source": [
    "License\n",
    "\n",
    "```\n",
    "Copyright (c) Facebook, Inc. and its affiliates.\n",
    "\n",
    "This source code is licensed under the MIT license found in the\n",
    "LICENSE file in the root directory of this source tree.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gsrdt9HooN9K"
   },
   "source": [
    "# Using CompilerGym environments with RLlib\n",
    "\n",
    "In this notebook we will use [RLlib](https://docs.ray.io/en/master/rllib.html) to train an agent for CompilerGym's [LLVM environment](https://facebookresearch.github.io/CompilerGym/llvm/index.html). RLlib is a popular library for scalable reinforcement learning, built on [Ray](https://docs.ray.io/en/master/index.html). It provides distributed implementations of several standard reinforcement learning algorithms.\n",
    "\n",
    "Our goal is not to produce the best agent, but to demonstrate how to integrate CompilerGym with RLlib. It will take about 20 minutes to work through. Let's get started!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "W8CSxbx5ovuF"
   },
   "source": [
    "## Installation\n",
    "\n",
    "We'll begin by installing the `compiler_gym` and `ray` packages:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cT3QDtxbf3Cr",
    "outputId": "399fced8-ec0c-4745-cf29-74dac40429f4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "compiler_gym version: 0.1.9\n",
      "ray version: 1.4.1\n",
      "gym version: 0.18.0\n",
      "Python 3.8.10\r\n"
     ]
    }
   ],
   "source": [
    "# Print the versions of the libraries that we are using:\n",
    "import compiler_gym\n",
    "import ray\n",
    "import gym\n",
    "\n",
    "print(\"compiler_gym version:\", compiler_gym.__version__)\n",
    "print(\"ray version:\", ray.__version__)\n",
    "print(\"gym version:\", gym.__version__)\n",
    "!python3 --version"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5Y2bt7GttpQ3"
   },
   "source": [
    "## Defining an Environment\n",
    "\n",
    "Next we will define the environment to use for our experiments. For the purposes of a simple demo we will apply two simplifying constraints to CompilerGym's LLVM environment:\n",
    "\n",
    "1. We will use only a small subset of the command line flag action space.\n",
    "2. We will clip the length of episodes to a maximum number of steps.\n",
    "\n",
    "To make things simple we will define a `make_env()` helper function to create our environment, and use the [compiler_gym.wrappers](https://facebookresearch.github.io/CompilerGym/compiler_gym/wrappers.html) API to implement these constraints. There is quite a lot going on in this cell, be sure to read through the comments for an explanation of what is going on!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "bNeq2sArf5pa"
   },
   "outputs": [],
   "source": [
    "#from compiler_gym.wrappers import ConstrainedCommandline, TimeLimit\n",
    "from ray import tune\n",
    "from compiler_gym.wrappers import ConstrainedCommandline, TimeLimit\n",
    "\n",
    "def make_env() -> compiler_gym.envs.CompilerEnv:\n",
    "    \"\"\"Make the reinforcement learning environment for this experiment.\"\"\"\n",
    "    # We will use LLVM as our base environment. Here we specify the observation\n",
    "    # space from this paper: https://arxiv.org/pdf/2003.00671.pdf and the total\n",
    "    # IR instruction count as our reward space, normalized against the \n",
    "    # performance of LLVM's -Oz policy.\n",
    "    env = gym.make(\n",
    "        \"llvm-v0\",\n",
    "        observation_space=\"Autophase\",\n",
    "        reward_space=\"IrInstructionCountOz\",\n",
    "    )\n",
    "    # Here we constrain the action space of the environment to use only a \n",
    "    # handful of command line flags from the full set. We do this to speed up\n",
    "    # learning by pruning the action space by hand. This also limits the \n",
    "    # potential improvements that the agent can achieve compared to using the \n",
    "    # full action space.\n",
    "    \n",
    "    env = ConstrainedCommandline(env, flags=[\n",
    "        \"-break-crit-edges\",\n",
    "        \"-early-cse-memssa\",\n",
    "        \"-gvn-hoist\",\n",
    "        \"-gvn\",\n",
    "        \"-instcombine\",\n",
    "        \"-instsimplify\",\n",
    "        \"-jump-threading\",\n",
    "        \"-loop-reduce\",\n",
    "        \"-loop-rotate\",\n",
    "        \"-loop-versioning\",\n",
    "        \"-mem2reg\",\n",
    "        \"-newgvn\",\n",
    "        \"-reg2mem\",\n",
    "        \"-simplifycfg\",\n",
    "        \"-sroa\",\n",
    "    ])\n",
    "    # Finally, we impose a time limit on the environment so that every episode\n",
    "    # for 5 steps or fewer. This is because the environment's task is continuous\n",
    "    # and no action is guaranteed to result in a terminal state. Adding a time\n",
    "    # limit means we don't have to worry about learning when an agent should \n",
    "    # stop, though again this limits the potential improvements that the agent\n",
    "    # can achieve compared to using an unbounded maximum episode length.\n",
    "    env = TimeLimit(env, max_episode_steps=5)\n",
    "    \n",
    "    return env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Eg84RXsDt5ey",
    "outputId": "8d1fa820-348e-400e-de6c-0a6d9a14d9bb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Action space: Commandline([-break-crit-edges -early-cse-memssa -gvn-hoist -gvn -instcombine -instsimplify -jump-threading -loop-reduce -loop-rotate -loop-versioning -mem2reg -newgvn -reg2mem -simplifycfg -sroa])\n",
      "Observation space: Box(0, 9223372036854775807, (56,), int64)\n",
      "Reward space: IrInstructionCountOz\n"
     ]
    }
   ],
   "source": [
    "# Let's create an environment and print a few attributes just to check that we \n",
    "# have everything set up the way that we would like.\n",
    "with make_env() as env:\n",
    "    print(\"Action space:\", env.action_space)\n",
    "    print(\"Observation space:\", env.observation_space)\n",
    "    print(\"Reward space:\", env.reward_space)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "39k80F8itTT_"
   },
   "source": [
    "## Datasets\n",
    "\n",
    "Now that we have an environment, we will need a set of programs to train on. In CompilerGym, these programs are called *benchmarks*. CompilerGym ships with [several sets of benchmarks](https://facebookresearch.github.io/CompilerGym/llvm/index.html#datasets). Here we will take a handful of benchmarks from the `npb-v0` dataset for training. We will then further divide this set into training and validation sets. We will use `chstone-v0` as a holdout test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "IV6-adNIhoGS",
    "outputId": "23afc569-58d1-443a-a4a7-e72b703152cc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of benchmarks for training: 50\n",
      "Number of benchmarks for validation: 5\n",
      "Number of benchmarks for testing: 12\n"
     ]
    }
   ],
   "source": [
    "from itertools import islice\n",
    "\n",
    "with make_env() as env:\n",
    "  # The two datasets we will be using:\n",
    "  npb = env.datasets[\"npb-v0\"]\n",
    "  chstone = env.datasets[\"chstone-v0\"]\n",
    "\n",
    "  # Each dataset has a `benchmarks()` method that returns an iterator over the\n",
    "  # benchmarks within the dataset. Here we will use iterator sliceing to grab a \n",
    "  # handful of benchmarks for training and validation.\n",
    "  train_benchmarks = list(islice(npb.benchmarks(), 55))\n",
    "  train_benchmarks, val_benchmarks = train_benchmarks[:50], train_benchmarks[50:]\n",
    "  # We will use the entire chstone-v0 dataset for testing.\n",
    "  test_benchmarks = list(chstone.benchmarks())\n",
    "\n",
    "print(\"Number of benchmarks for training:\", len(train_benchmarks))\n",
    "print(\"Number of benchmarks for validation:\", len(val_benchmarks))\n",
    "print(\"Number of benchmarks for testing:\", len(test_benchmarks))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZW0sfMhjv8Kg"
   },
   "source": [
    "## Registering the environment with RLlib\n",
    "\n",
    "Now that we have our environment and training benchmarks, we can register the environment for use with RLlib. To do this we will define a second `make_training_env()` helper that uses the [CycleOverBenchmarks](https://facebookresearch.github.io/CompilerGym/compiler_gym/wrappers.html#compiler_gym.wrappers.CycleOverBenchmarks) wrapper to ensure that the environment uses all of the training benchmarks. We then call `tune.register_env()`, assining the environment a name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "-UgFKvTkv64L"
   },
   "outputs": [],
   "source": [
    "from compiler_gym.wrappers import CycleOverBenchmarks\n",
    "\n",
    "def make_training_env(*args) -> compiler_gym.envs.CompilerEnv:\n",
    "  \"\"\"Make a reinforcement learning environment that cycles over the\n",
    "  set of training benchmarks in use.\n",
    "  \"\"\"\n",
    "  del args  # Unused env_config argument passed by ray\n",
    "  return CycleOverBenchmarks(make_env(), train_benchmarks)\n",
    "\n",
    "tune.register_env(\"compiler_gym\", make_training_env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6CVCAIrpyKa4",
    "outputId": "76537a51-91e7-46c5-ab7c-e71122be04c6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "benchmark://npb-v0/1\n",
      "benchmark://npb-v0/2\n",
      "benchmark://npb-v0/3\n"
     ]
    }
   ],
   "source": [
    "# Lets cycle through a few calls to reset() to demonstrate that this environment\n",
    "# selects a new benchmark for each episode.\n",
    "with make_training_env() as env:\n",
    "  env.reset()\n",
    "  print(env.benchmark)\n",
    "  env.reset()\n",
    "  print(env.benchmark)\n",
    "  env.reset()\n",
    "  print(env.benchmark)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IpR63LcOuRRz"
   },
   "source": [
    "## Run the training loop\n",
    "\n",
    "Now that we have the environment set up, let's run a training loop. Here will use RLlib's [Proximal Policy Optimization](https://docs.ray.io/en/master/rllib-algorithms.html#ppo) implementation, and run a very short training loop just for demonstative purposes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "KYGUxq6GhXZL",
    "outputId": "6ce9e698-6fd2-442d-976d-40c4c0a0902f"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 2.9/11.2 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 0/12 CPUs, 0/0 GPUs, 0.0/5.5 GiB heap, 0.0/2.75 GiB objects<br>Result logdir: /home/phesse/ray_results/PPO_2021-08-08_17-01-13<br>Number of trials: 1/1 (1 PENDING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                  </th><th>status  </th><th>loc  </th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_compiler_gym_25894_00000</td><td>PENDING </td><td>     </td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=5637)\u001b[0m WARNING: CPU random generator seem to be failing, disabling hardware random number generation\n",
      "\u001b[2m\u001b[36m(pid=5637)\u001b[0m WARNING: RDRND generated: 0xffffffff 0xffffffff 0xffffffff 0xffffffff\n",
      "\u001b[2m\u001b[36m(pid=5637)\u001b[0m 2021-08-08 17:01:15,505\tINFO trainer.py:696 -- Current log_level is WARN. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.\n",
      "\u001b[2m\u001b[36m(pid=5637)\u001b[0m 2021-08-08 17:01:15,520\tWARNING ppo.py:135 -- `train_batch_size` (40) cannot be achieved with your other settings (num_workers=8 num_envs_per_worker=1 rollout_fragment_length=40)! Auto-adjusting `rollout_fragment_length` to 5.\n",
      "\u001b[2m\u001b[36m(pid=5640)\u001b[0m WARNING: CPU random generator seem to be failing, disabling hardware random number generation\n",
      "\u001b[2m\u001b[36m(pid=5640)\u001b[0m WARNING: RDRND generated: 0xffffffff 0xffffffff 0xffffffff 0xffffffff\n",
      "\u001b[2m\u001b[36m(pid=5641)\u001b[0m WARNING: CPU random generator seem to be failing, disabling hardware random number generation\n",
      "\u001b[2m\u001b[36m(pid=5641)\u001b[0m WARNING: RDRND generated: 0xffffffff 0xffffffff 0xffffffff 0xffffffff\n",
      "\u001b[2m\u001b[36m(pid=5634)\u001b[0m WARNING: CPU random generator seem to be failing, disabling hardware random number generation\n",
      "\u001b[2m\u001b[36m(pid=5634)\u001b[0m WARNING: RDRND generated: 0xffffffff 0xffffffff 0xffffffff 0xffffffff\n",
      "\u001b[2m\u001b[36m(pid=5633)\u001b[0m WARNING: CPU random generator seem to be failing, disabling hardware random number generation\n",
      "\u001b[2m\u001b[36m(pid=5633)\u001b[0m WARNING: RDRND generated: 0xffffffff 0xffffffff 0xffffffff 0xffffffff\n",
      "\u001b[2m\u001b[36m(pid=5630)\u001b[0m WARNING: CPU random generator seem to be failing, disabling hardware random number generation\n",
      "\u001b[2m\u001b[36m(pid=5630)\u001b[0m WARNING: RDRND generated: 0xffffffff 0xffffffff 0xffffffff 0xffffffff\n",
      "\u001b[2m\u001b[36m(pid=5635)\u001b[0m WARNING: CPU random generator seem to be failing, disabling hardware random number generation\n",
      "\u001b[2m\u001b[36m(pid=5635)\u001b[0m WARNING: RDRND generated: 0xffffffff 0xffffffff 0xffffffff 0xffffffff\n",
      "\u001b[2m\u001b[36m(pid=5631)\u001b[0m WARNING: CPU random generator seem to be failing, disabling hardware random number generation\n",
      "\u001b[2m\u001b[36m(pid=5631)\u001b[0m WARNING: RDRND generated: 0xffffffff 0xffffffff 0xffffffff 0xffffffff\n",
      "\u001b[2m\u001b[36m(pid=5632)\u001b[0m WARNING: CPU random generator seem to be failing, disabling hardware random number generation\n",
      "\u001b[2m\u001b[36m(pid=5632)\u001b[0m WARNING: RDRND generated: 0xffffffff 0xffffffff 0xffffffff 0xffffffff\n",
      "\u001b[2m\u001b[36m(pid=5637)\u001b[0m 2021-08-08 17:01:20,205\tWARNING util.py:53 -- Install gputil for GPU system monitoring.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_compiler_gym_25894_00000:\n",
      "  agent_timesteps_total: 40\n",
      "  custom_metrics: {}\n",
      "  date: 2021-08-08_17-01-20\n",
      "  done: false\n",
      "  episode_len_mean: 5.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.9186046511627907\n",
      "  episode_reward_mean: 0.8066860465116279\n",
      "  episode_reward_min: 0.372093023255814\n",
      "  episodes_this_iter: 8\n",
      "  episodes_total: 8\n",
      "  experiment_id: bca003b6cdbb4523933474353d3f5c1f\n",
      "  hostname: phesse\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 0.2\n",
      "          cur_lr: 5.0e-05\n",
      "          entropy: 2.6838157176971436\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.024068277329206467\n",
      "          policy_loss: -0.1702536791563034\n",
      "          total_loss: -0.013393327593803406\n",
      "          vf_explained_var: 0.1914040446281433\n",
      "          vf_loss: 0.15204669535160065\n",
      "    num_agent_steps_sampled: 40\n",
      "    num_steps_sampled: 40\n",
      "    num_steps_trained: 40\n",
      "  iterations_since_restore: 1\n",
      "  node_ip: 192.168.1.18\n",
      "  num_healthy_workers: 8\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 41.1\n",
      "    ram_util_percent: 45.5\n",
      "  pid: 5637\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.13076265652974445\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 9.599924087524414\n",
      "    mean_inference_ms: 1.883039871851603\n",
      "    mean_raw_obs_processing_ms: 14.205033580462137\n",
      "  time_since_restore: 0.3696608543395996\n",
      "  time_this_iter_s: 0.3696608543395996\n",
      "  time_total_s: 0.3696608543395996\n",
      "  timers:\n",
      "    learn_throughput: 234.673\n",
      "    learn_time_ms: 170.45\n",
      "    sample_throughput: 211.288\n",
      "    sample_time_ms: 189.315\n",
      "    update_time_ms: 2.677\n",
      "  timestamp: 1628460080\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 40\n",
      "  training_iteration: 1\n",
      "  trial_id: '25894_00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 5.1/11.1 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 9.0/12 CPUs, 0/0 GPUs, 0.0/5.5 GiB heap, 0.0/2.75 GiB objects<br>Result logdir: /home/phesse/ray_results/PPO_2021-08-08_17-01-13<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                  </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_compiler_gym_25894_00000</td><td>RUNNING </td><td>192.168.1.18:5637</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">        0.369661</td><td style=\"text-align: right;\">  40</td><td style=\"text-align: right;\">0.806686</td><td style=\"text-align: right;\">            0.918605</td><td style=\"text-align: right;\">            0.372093</td><td style=\"text-align: right;\">                 5</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_compiler_gym_25894_00000:\n",
      "  agent_timesteps_total: 600\n",
      "  custom_metrics: {}\n",
      "  date: 2021-08-08_17-01-25\n",
      "  done: false\n",
      "  episode_len_mean: 5.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 13.431818181818183\n",
      "  episode_reward_mean: 1.5202102148724421\n",
      "  episode_reward_min: -0.39999999999999997\n",
      "  episodes_this_iter: 8\n",
      "  episodes_total: 120\n",
      "  experiment_id: bca003b6cdbb4523933474353d3f5c1f\n",
      "  hostname: phesse\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 2.278125\n",
      "          cur_lr: 5.0e-05\n",
      "          entropy: 2.156273126602173\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.019648782908916473\n",
      "          policy_loss: -0.12628258764743805\n",
      "          total_loss: 0.023693528026342392\n",
      "          vf_explained_var: 0.7793610095977783\n",
      "          vf_loss: 0.10521373897790909\n",
      "    num_agent_steps_sampled: 600\n",
      "    num_steps_sampled: 600\n",
      "    num_steps_trained: 600\n",
      "  iterations_since_restore: 15\n",
      "  node_ip: 192.168.1.18\n",
      "  num_healthy_workers: 8\n",
      "  off_policy_estimator: {}\n",
      "  perf: {}\n",
      "  pid: 5637\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.072677891272905\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 3.314915839398724\n",
      "    mean_inference_ms: 1.685386768143918\n",
      "    mean_raw_obs_processing_ms: 23.694462897237926\n",
      "  time_since_restore: 5.108668327331543\n",
      "  time_this_iter_s: 0.22418951988220215\n",
      "  time_total_s: 5.108668327331543\n",
      "  timers:\n",
      "    learn_throughput: 256.519\n",
      "    learn_time_ms: 155.934\n",
      "    sample_throughput: 219.467\n",
      "    sample_time_ms: 182.26\n",
      "    update_time_ms: 1.934\n",
      "  timestamp: 1628460085\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 600\n",
      "  training_iteration: 15\n",
      "  trial_id: '25894_00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 5.1/11.1 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 9.0/12 CPUs, 0/0 GPUs, 0.0/5.5 GiB heap, 0.0/2.75 GiB objects (0.0/1.0 CPU_group_7_a06589562964f8538b55d1d723079a4f, 0.0/1.0 CPU_group_6_a06589562964f8538b55d1d723079a4f, 0.0/9.0 CPU_group_a06589562964f8538b55d1d723079a4f, 0.0/1.0 CPU_group_1_a06589562964f8538b55d1d723079a4f, 0.0/1.0 CPU_group_2_a06589562964f8538b55d1d723079a4f, 0.0/1.0 CPU_group_5_a06589562964f8538b55d1d723079a4f, 0.0/1.0 CPU_group_0_a06589562964f8538b55d1d723079a4f, 0.0/1.0 CPU_group_3_a06589562964f8538b55d1d723079a4f, 0.0/1.0 CPU_group_8_a06589562964f8538b55d1d723079a4f, 0.0/1.0 CPU_group_4_a06589562964f8538b55d1d723079a4f)<br>Result logdir: /home/phesse/ray_results/PPO_2021-08-08_17-01-13<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                  </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_compiler_gym_25894_00000</td><td>RUNNING </td><td>192.168.1.18:5637</td><td style=\"text-align: right;\">    15</td><td style=\"text-align: right;\">         5.10867</td><td style=\"text-align: right;\"> 600</td><td style=\"text-align: right;\"> 1.52021</td><td style=\"text-align: right;\">             13.4318</td><td style=\"text-align: right;\">                -0.4</td><td style=\"text-align: right;\">                 5</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_compiler_gym_25894_00000:\n",
      "  agent_timesteps_total: 1200\n",
      "  custom_metrics: {}\n",
      "  date: 2021-08-08_17-01-30\n",
      "  done: false\n",
      "  episode_len_mean: 5.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 148.0\n",
      "  episode_reward_mean: 4.469694483576115\n",
      "  episode_reward_min: -39.0\n",
      "  episodes_this_iter: 8\n",
      "  episodes_total: 240\n",
      "  experiment_id: bca003b6cdbb4523933474353d3f5c1f\n",
      "  hostname: phesse\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 2.562890625\n",
      "          cur_lr: 5.0e-05\n",
      "          entropy: 2.0724945068359375\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.006857914384454489\n",
      "          policy_loss: -0.07182396203279495\n",
      "          total_loss: -0.023833811283111572\n",
      "          vf_explained_var: 0.6915101408958435\n",
      "          vf_loss: 0.03041406348347664\n",
      "    num_agent_steps_sampled: 1200\n",
      "    num_steps_sampled: 1200\n",
      "    num_steps_trained: 1200\n",
      "  iterations_since_restore: 30\n",
      "  node_ip: 192.168.1.18\n",
      "  num_healthy_workers: 8\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 49.0\n",
      "    ram_util_percent: 46.6\n",
      "  pid: 5637\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.0640122948073612\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 2.4917396880239266\n",
      "    mean_inference_ms: 1.5859367609112787\n",
      "    mean_raw_obs_processing_ms: 27.344577220136912\n",
      "  time_since_restore: 10.0923171043396\n",
      "  time_this_iter_s: 0.24954557418823242\n",
      "  time_total_s: 10.0923171043396\n",
      "  timers:\n",
      "    learn_throughput: 263.618\n",
      "    learn_time_ms: 151.735\n",
      "    sample_throughput: 243.213\n",
      "    sample_time_ms: 164.465\n",
      "    update_time_ms: 1.842\n",
      "  timestamp: 1628460090\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 1200\n",
      "  training_iteration: 30\n",
      "  trial_id: '25894_00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 5.2/11.1 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 9.0/12 CPUs, 0/0 GPUs, 0.0/5.5 GiB heap, 0.0/2.75 GiB objects (0.0/1.0 CPU_group_7_a06589562964f8538b55d1d723079a4f, 0.0/1.0 CPU_group_6_a06589562964f8538b55d1d723079a4f, 0.0/9.0 CPU_group_a06589562964f8538b55d1d723079a4f, 0.0/1.0 CPU_group_1_a06589562964f8538b55d1d723079a4f, 0.0/1.0 CPU_group_2_a06589562964f8538b55d1d723079a4f, 0.0/1.0 CPU_group_5_a06589562964f8538b55d1d723079a4f, 0.0/1.0 CPU_group_0_a06589562964f8538b55d1d723079a4f, 0.0/1.0 CPU_group_3_a06589562964f8538b55d1d723079a4f, 0.0/1.0 CPU_group_8_a06589562964f8538b55d1d723079a4f, 0.0/1.0 CPU_group_4_a06589562964f8538b55d1d723079a4f)<br>Result logdir: /home/phesse/ray_results/PPO_2021-08-08_17-01-13<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                  </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_compiler_gym_25894_00000</td><td>RUNNING </td><td>192.168.1.18:5637</td><td style=\"text-align: right;\">    30</td><td style=\"text-align: right;\">         10.0923</td><td style=\"text-align: right;\">1200</td><td style=\"text-align: right;\"> 4.46969</td><td style=\"text-align: right;\">                 148</td><td style=\"text-align: right;\">                 -39</td><td style=\"text-align: right;\">                 5</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_compiler_gym_25894_00000:\n",
      "  agent_timesteps_total: 1840\n",
      "  custom_metrics: {}\n",
      "  date: 2021-08-08_17-01-36\n",
      "  done: false\n",
      "  episode_len_mean: 5.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 139.0\n",
      "  episode_reward_mean: 11.810474284824522\n",
      "  episode_reward_min: 0.05555555555555555\n",
      "  episodes_this_iter: 8\n",
      "  episodes_total: 368\n",
      "  experiment_id: bca003b6cdbb4523933474353d3f5c1f\n",
      "  hostname: phesse\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.4416259765625001\n",
      "          cur_lr: 5.0e-05\n",
      "          entropy: 1.6911900043487549\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.022438909858465195\n",
      "          policy_loss: -0.09711308777332306\n",
      "          total_loss: 0.4777809977531433\n",
      "          vf_explained_var: -0.6321806907653809\n",
      "          vf_loss: 0.5425456166267395\n",
      "    num_agent_steps_sampled: 1840\n",
      "    num_steps_sampled: 1840\n",
      "    num_steps_trained: 1840\n",
      "  iterations_since_restore: 46\n",
      "  node_ip: 192.168.1.18\n",
      "  num_healthy_workers: 8\n",
      "  off_policy_estimator: {}\n",
      "  perf: {}\n",
      "  pid: 5637\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06126216955286741\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 2.3782466090177294\n",
      "    mean_inference_ms: 1.5271219305876127\n",
      "    mean_raw_obs_processing_ms: 27.275770769858717\n",
      "  time_since_restore: 15.027718305587769\n",
      "  time_this_iter_s: 0.34296703338623047\n",
      "  time_total_s: 15.027718305587769\n",
      "  timers:\n",
      "    learn_throughput: 277.001\n",
      "    learn_time_ms: 144.404\n",
      "    sample_throughput: 243.359\n",
      "    sample_time_ms: 164.366\n",
      "    update_time_ms: 1.749\n",
      "  timestamp: 1628460096\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 1840\n",
      "  training_iteration: 46\n",
      "  trial_id: '25894_00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 5.2/11.1 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 9.0/12 CPUs, 0/0 GPUs, 0.0/5.5 GiB heap, 0.0/2.75 GiB objects (0.0/1.0 CPU_group_7_a06589562964f8538b55d1d723079a4f, 0.0/1.0 CPU_group_0_a06589562964f8538b55d1d723079a4f, 0.0/1.0 CPU_group_3_a06589562964f8538b55d1d723079a4f, 0.0/9.0 CPU_group_a06589562964f8538b55d1d723079a4f, 0.0/1.0 CPU_group_1_a06589562964f8538b55d1d723079a4f, 0.0/1.0 CPU_group_5_a06589562964f8538b55d1d723079a4f, 0.0/1.0 CPU_group_8_a06589562964f8538b55d1d723079a4f, 0.0/1.0 CPU_group_4_a06589562964f8538b55d1d723079a4f, 0.0/1.0 CPU_group_6_a06589562964f8538b55d1d723079a4f, 0.0/1.0 CPU_group_2_a06589562964f8538b55d1d723079a4f)<br>Result logdir: /home/phesse/ray_results/PPO_2021-08-08_17-01-13<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                  </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_compiler_gym_25894_00000</td><td>RUNNING </td><td>192.168.1.18:5637</td><td style=\"text-align: right;\">    46</td><td style=\"text-align: right;\">         15.0277</td><td style=\"text-align: right;\">1840</td><td style=\"text-align: right;\"> 11.8105</td><td style=\"text-align: right;\">                 139</td><td style=\"text-align: right;\">           0.0555556</td><td style=\"text-align: right;\">                 5</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_compiler_gym_25894_00000:\n",
      "  agent_timesteps_total: 2520\n",
      "  custom_metrics: {}\n",
      "  date: 2021-08-08_17-01-39\n",
      "  done: true\n",
      "  episode_len_mean: 5.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 12.045454545454545\n",
      "  episode_reward_mean: 1.8014903542005625\n",
      "  episode_reward_min: 0.42118863049095606\n",
      "  episodes_this_iter: 8\n",
      "  episodes_total: 504\n",
      "  experiment_id: bca003b6cdbb4523933474353d3f5c1f\n",
      "  hostname: phesse\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.3684184074401857\n",
      "          cur_lr: 5.0e-05\n",
      "          entropy: 1.7659261226654053\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.010017195716500282\n",
      "          policy_loss: -0.04969358816742897\n",
      "          total_loss: 13.75989818572998\n",
      "          vf_explained_var: 0.4776380658149719\n",
      "          vf_loss: 13.79588508605957\n",
      "    num_agent_steps_sampled: 2520\n",
      "    num_steps_sampled: 2520\n",
      "    num_steps_trained: 2520\n",
      "  iterations_since_restore: 63\n",
      "  node_ip: 192.168.1.18\n",
      "  num_healthy_workers: 8\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 19.8\n",
      "    ram_util_percent: 47.1\n",
      "  pid: 5637\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.059985499428920584\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 2.3021298078632895\n",
      "    mean_inference_ms: 1.498715089395398\n",
      "    mean_raw_obs_processing_ms: 23.54443283370685\n",
      "  time_since_restore: 18.548845767974854\n",
      "  time_this_iter_s: 0.18659663200378418\n",
      "  time_total_s: 18.548845767974854\n",
      "  timers:\n",
      "    learn_throughput: 280.934\n",
      "    learn_time_ms: 142.382\n",
      "    sample_throughput: 1345.672\n",
      "    sample_time_ms: 29.725\n",
      "    update_time_ms: 1.803\n",
      "  timestamp: 1628460099\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 2520\n",
      "  training_iteration: 63\n",
      "  trial_id: '25894_00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 5.2/11.1 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 0/12 CPUs, 0/0 GPUs, 0.0/5.5 GiB heap, 0.0/2.75 GiB objects (0.0/1.0 CPU_group_7_a06589562964f8538b55d1d723079a4f, 0.0/1.0 CPU_group_0_a06589562964f8538b55d1d723079a4f, 0.0/1.0 CPU_group_3_a06589562964f8538b55d1d723079a4f, 0.0/9.0 CPU_group_a06589562964f8538b55d1d723079a4f, 0.0/1.0 CPU_group_1_a06589562964f8538b55d1d723079a4f, 0.0/1.0 CPU_group_5_a06589562964f8538b55d1d723079a4f, 0.0/1.0 CPU_group_8_a06589562964f8538b55d1d723079a4f, 0.0/1.0 CPU_group_4_a06589562964f8538b55d1d723079a4f, 0.0/1.0 CPU_group_6_a06589562964f8538b55d1d723079a4f, 0.0/1.0 CPU_group_2_a06589562964f8538b55d1d723079a4f)<br>Result logdir: /home/phesse/ray_results/PPO_2021-08-08_17-01-13<br>Number of trials: 1/1 (1 TERMINATED)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                  </th><th>status    </th><th>loc  </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_compiler_gym_25894_00000</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\">    63</td><td style=\"text-align: right;\">         18.5488</td><td style=\"text-align: right;\">2520</td><td style=\"text-align: right;\"> 1.80149</td><td style=\"text-align: right;\">             12.0455</td><td style=\"text-align: right;\">            0.421189</td><td style=\"text-align: right;\">                 5</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-08-08 17:01:40,722\tINFO tune.py:549 -- Total run time: 27.43 seconds (26.49 seconds for the tuning loop).\n"
     ]
    }
   ],
   "source": [
    "import ray\n",
    "from ray.rllib.agents.ppo import PPOTrainer\n",
    "\n",
    "# (Re)Start the ray runtime.\n",
    "if ray.is_initialized():\n",
    "  ray.shutdown()\n",
    "ray.init(include_dashboard=False, ignore_reinit_error=True)\n",
    "\n",
    "tune.register_env(\"compiler_gym\", make_training_env)\n",
    "\n",
    "analysis = tune.run(\n",
    "    PPOTrainer,\n",
    "    checkpoint_at_end=True,\n",
    "    stop={\n",
    "        \"episodes_total\": 500,\n",
    "    },\n",
    "    config={\n",
    "        \"seed\": None,\n",
    "        \"framework\": \"torch\",\n",
    "        \"num_workers\": 8,\n",
    "        # Specify the environment to use, where \"compiler_gym\" is the name we \n",
    "        # passed to tune.register_env().\n",
    "        \"env\": \"compiler_gym\",\n",
    "        # Reduce the size of the batch/trajectory lengths to match our short \n",
    "        # training run.\n",
    "        \"rollout_fragment_length\": 40,\n",
    "        \"train_batch_size\": 40,\n",
    "        \"sgd_minibatch_size\": 40,\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CFDxXezNuXdy"
   },
   "source": [
    "## Evaluate the agent\n",
    "\n",
    "After running the training loop we can create a new agent that has exploration disabled, restore it from the training checkpoint, and then use it for running inference tests."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4mYoBhSEi26c",
    "outputId": "2dae3ec2-9d0e-4919-86b9-10362d9dd5ee"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-08-08 17:08:08,912\tINFO trainer.py:671 -- Tip: set framework=tfe or the --eager flag to enable TensorFlow eager execution\n",
      "2021-08-08 17:08:08,913\tINFO trainer.py:696 -- Current log_level is WARN. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.\n",
      "\u001b[2m\u001b[36m(pid=5638)\u001b[0m WARNING: CPU random generator seem to be failing, disabling hardware random number generation\n",
      "\u001b[2m\u001b[36m(pid=5638)\u001b[0m WARNING: RDRND generated: 0xffffffff 0xffffffff 0xffffffff 0xffffffff\n",
      "2021-08-08 17:08:13,625\tWARNING util.py:53 -- Install gputil for GPU system monitoring.\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'items'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-6013d972b738>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     19\u001b[0m )\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrestore\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcheckpoint\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/ray/tune/trainable.py\u001b[0m in \u001b[0;36mrestore\u001b[0;34m(self, checkpoint_path)\u001b[0m\n\u001b[1;32m    370\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_checkpoint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcheckpoint_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    371\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 372\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_checkpoint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcheckpoint_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    373\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_time_since_restore\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    374\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_timesteps_since_restore\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/ray/rllib/agents/trainer.py\u001b[0m in \u001b[0;36mload_checkpoint\u001b[0;34m(self, checkpoint_path)\u001b[0m\n\u001b[1;32m    761\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mload_checkpoint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcheckpoint_path\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    762\u001b[0m         \u001b[0mextra_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcheckpoint_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"rb\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 763\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__setstate__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mextra_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    764\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    765\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mDeveloperAPI\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/ray/rllib/agents/trainer_template.py\u001b[0m in \u001b[0;36m__setstate__\u001b[0;34m(self, state)\u001b[0m\n\u001b[1;32m    221\u001b[0m         \u001b[0;34m@\u001b[0m\u001b[0moverride\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mTrainer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    222\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0m__setstate__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 223\u001b[0;31m             \u001b[0mTrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__setstate__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    224\u001b[0m             self.train_exec_impl.shared_metrics.get().restore(\n\u001b[1;32m    225\u001b[0m                 state[\"train_exec_impl\"])\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/ray/rllib/agents/trainer.py\u001b[0m in \u001b[0;36m__setstate__\u001b[0;34m(self, state)\u001b[0m\n\u001b[1;32m   1388\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__setstate__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1389\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;34m\"worker\"\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mstate\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"workers\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1390\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mworkers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlocal_worker\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrestore\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"worker\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1391\u001b[0m             \u001b[0mremote_state\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mray\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"worker\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1392\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mr\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mworkers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mremote_workers\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/ray/rllib/evaluation/rollout_worker.py\u001b[0m in \u001b[0;36mrestore\u001b[0;34m(self, objs)\u001b[0m\n\u001b[1;32m   1097\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msync_filters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobjs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"filters\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1098\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mpid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mobjs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"state\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1099\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpolicy_map\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpid\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_state\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1100\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1101\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mDeveloperAPI\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/ray/rllib/policy/tf_policy.py\u001b[0m in \u001b[0;36mset_state\u001b[0;34m(self, state)\u001b[0m\n\u001b[1;32m    510\u001b[0m         \u001b[0moptimizer_vars\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"_optimizer_variables\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    511\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0moptimizer_vars\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 512\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_optimizer_variables\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moptimizer_vars\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    513\u001b[0m         \u001b[0;31m# Then the Policy's (NN) weights.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    514\u001b[0m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_state\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/ray/experimental/tf_utils.py\u001b[0m in \u001b[0;36mset_weights\u001b[0;34m(self, new_weights)\u001b[0m\n\u001b[1;32m    180\u001b[0m                 \u001b[0mvar\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0massign\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew_weights\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    181\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 182\u001b[0;31m             \u001b[0massign_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_assign_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew_weights\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    183\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0massign_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    184\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/ray/experimental/tf_utils.py\u001b[0m in \u001b[0;36m_assign_weights\u001b[0;34m(self, weights)\u001b[0m\n\u001b[1;32m    206\u001b[0m             \u001b[0massignable\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mremove\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    207\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 208\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mweights\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    209\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32min\u001b[0m \u001b[0massignable\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    210\u001b[0m                 \u001b[0massign\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'list' object has no attribute 'items'"
     ]
    }
   ],
   "source": [
    "agent = PPOTrainer(\n",
    "    env=\"compiler_gym\",\n",
    "    config={\n",
    "        \"num_workers\": 1,\n",
    "        \"seed\": None,\n",
    "        # For inference we disable the stocastic exploration that is used during \n",
    "        # training.\n",
    "        \"explore\": False,\n",
    "    },\n",
    ")\n",
    "\n",
    "# We only made a single checkpoint at the end of training, so restore that. In\n",
    "# practice we may have many checkpoints that we will select from using \n",
    "# performance on the validation set.\n",
    "checkpoint = analysis.get_best_checkpoint(\n",
    "    metric=\"episode_reward_mean\", \n",
    "    mode=\"max\", \n",
    "    trial=analysis.trials[0]\n",
    ")\n",
    "\n",
    "agent.restore(checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "aWeLEVYZjVuM",
    "outputId": "ee293b46-650f-4fbb-8ff4-4414eb4140c5"
   },
   "outputs": [],
   "source": [
    "# Lets define a helper function to make it easy to evaluate the agent's \n",
    "# performance on a set of benchmarks.\n",
    "\n",
    "def run_agent_on_benchmarks(benchmarks):\n",
    "  \"\"\"Run agent on a list of benchmarks and return a list of cumulative rewards.\"\"\"\n",
    "  with make_env() as env:\n",
    "    rewards = []\n",
    "    for i, benchmark in enumerate(benchmarks, start=1):\n",
    "        observation, done = env.reset(benchmark=benchmark), False\n",
    "        while not done:\n",
    "            action = agent.compute_action(observation)\n",
    "            observation, _, done, _ = env.step(action)\n",
    "        rewards.append(env.episode_reward)\n",
    "        print(f\"[{i}/{len(benchmarks)}] {env.state}\")\n",
    "\n",
    "  return rewards\n",
    "\n",
    "# Evaluate agent performance on the validation set.\n",
    "val_rewards = run_agent_on_benchmarks(val_benchmarks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "DEc872g10UmF",
    "outputId": "96170474-0742-4e7c-9d53-4d2fc094f9fb"
   },
   "outputs": [],
   "source": [
    "# Evaluate agent performance on the holdout test set.\n",
    "test_rewards = run_agent_on_benchmarks(test_benchmarks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 383
    },
    "id": "Oo3Dn360EE5A",
    "outputId": "55879094-dcc6-4d43-f385-d71db1300efe"
   },
   "outputs": [],
   "source": [
    "# Finally lets plot our results to see how we did!\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "def plot_results(x, y, name, ax):\n",
    "  plt.sca(ax)\n",
    "  plt.bar(range(len(y)), y)\n",
    "  plt.ylabel(\"Reward (higher is better)\")\n",
    "  plt.xticks(range(len(x)), x, rotation = 90)\n",
    "  plt.title(f\"Performance on {name} set\")\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2)\n",
    "fig.set_size_inches(13, 3)\n",
    "plot_results(val_benchmarks, val_rewards, \"val\", ax1)\n",
    "plot_results(test_benchmarks, test_rewards, \"test\", ax2)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Z1lXpJbA04Ji"
   },
   "source": [
    "That's it for this demonstration! Check out the [documentation site](https://facebookresearch.github.io/CompilerGym/) for more details, API reference, and more. If you can encounter any problems, please [file an issue](https://github.com/facebookresearch/CompilerGym/issues)."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "rllib-example.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
